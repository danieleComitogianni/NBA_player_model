# NBA Player Performance Data Processing Pipeline

This repository contains a series of Python scripts designed to process, clean, feature engineer, and merge NBA player and team data spanning multiple seasons (2014-2015 to 2023-2024). The goal is to create a final dataset suitable for machine learning modeling, specifically for predicting player performance.

## Execution Order and Script Descriptions

The scripts should be executed in the following order:

**1. `season_YYYY_YYYY.py` (Run for each season from 2014-2015 to 2023-2024)**

- **Goal:** Initial processing and merging of raw player data for a _single_ season.
- **Inputs:**
  - `../../data/DARKO.xlsx`: Contains DARKO advanced player metrics.
  - `../../data/boxscore_YYYY.xlsx`: Contains raw player box score data for the specific season (e.g., `boxscore_2014.xlsx`).
  - `../../data/lebron_processed.xlsx`: Contains LEBRON advanced player metrics.
- **Actions:**
  1.  Loads the three input dataframes.
  2.  Filters the `DARKO` dataframe to keep only the relevant season's data (e.g., `season >= 2015` for the 2014-2015 season script).
  3.  Adds a `season` column to the box score data.
  4.  Cleans player names (lowercase, removes punctuation, suffixes like 'jr', 'sr') in both box score and DARKO data for consistent merging.
  5.  Renames box score columns for clarity and consistency.
  6.  Drops redundant columns (`nba_id`, `team_name`) from the filtered DARKO data before merging.
  7.  Merges the processed box score data with the filtered DARKO data using `player_name` and `season`.
  8.  Identifies and reports players/rows where DARKO metrics are missing after the merge.
  9.  Merges the result with the LEBRON data using `player_name` and `season`.
  10. Manually imputes (fills in) missing LEBRON data for a predefined list of players for the specific season (hardcoded values).
  11. Identifies and reports players/rows where LEBRON metrics are still missing after imputation.
- **Output:**
  - `../../data/processed_YYYY+1.xlsx` (e.g., `season_2014_2015.py` outputs `processed_2015.xlsx`). This file contains the merged and partially cleaned player data for one season.

**2. `feature_engineering.py`**

- **Goal:** Generate time-series features (lagged averages, rolling stats, season averages) for each player based on their past performance within each season.
- **Inputs:**
  - `../../data/processed_YYYY.xlsx` (for each year from 2015 to 2024, generated by the previous step).
- **Actions:**
  1.  Iterates through the seasons 2015 to 2024.
  2.  Loads the corresponding `processed_YYYY.xlsx` file.
  3.  **Rest Days Processing:** Converts the `rest_days` column (which might contain strings like '3+') into a numeric format (`rest_days_numeric`), creates a binary flag `extended_absence` (if rest days > 9), and a binary flag `is_b2b` (if rest days == 0). Drops the original `rest_days` column.
  4.  **Lagged Base Stats:** Calculates lagged moving averages for basic box score stats (FG, FGA, PTS, MIN, etc.) using:
      - Previous game's value (`rolling_1_`).
      - 3, 5, and 10-game Exponentially Weighted Moving Averages (EWMA) (`ewma_3_`, `ewma_5_`, `ewma_10_`). All averages are _lagged_ (they use data _before_ the current game).
  5.  **Derived Stats Calculation:** Computes advanced efficiency and per-minute metrics for each game based on that game's stats (FG%, 3P%, eFG%, TS%, TSA, PTS_per36, TOT_per36, A_per36, GmSc).
  6.  **Lagged Derived Stats:** Calculates lagged moving averages for these newly derived stats using:
      - Previous game's value (`rolling_1_`).
      - 3, 5, 10, and 20-game EWMA (`ewma_3_`, `ewma_5_`, `ewma_10_`, `ewma_20_`).
  7.  **Season Averages:** Calculates the player's expanding average for key metrics (`MIN`, `PTS`, `FG%`, `GmSc`, etc.) _up to the previous game_ within the season (`season_avg_`).
  8.  Merges all newly created lagged features back into the dataframe for that season.
- **Output:**
  - `../../data/feature_engineer/feature_engineered_YYYY.xlsx` (for each year from 2015 to 2024). This file contains the player data augmented with lagged performance features.

**3. `team_level_stats.py`**

- **Goal:** Process team-level box scores to calculate rolling team averages, opponent averages, league context, and matchup advantages _prior_ to each game.
- **Inputs:**
  - `../../data/*_NBA_Box_Score_Team-Stats.xlsx` (Raw team box score files for each season).
- **Actions:**
  1.  Iterates through each season's team box score file.
  2.  Selects relevant team stats (FG, PTS, PACE, OEFF, DEFF, Spread, Total, etc.).
  3.  Calculates the implied team total based on the opening spread and total.
  4.  Calculates cumulative league averages for OEFF, DEFF, and PACE _prior_ to each game day.
  5.  Calculates game-specific assist rate (AST/FG) and rebounding rates (ORR, DRR).
  6.  Calculates rolling team averages (OEFF, DEFF, PACE, AST Rate, ORR, DRR) based on all games _before_ the current one for that team in the season.
  7.  For each game, identifies the opponent and retrieves the opponent's pre-game rolling averages.
  8.  Calculates _predicted_ adjusted Offensive and Defensive Efficiency (`pred_adj_oeff`, `pred_adj_deff`) for the upcoming game based on the team's average, the opponent's average, and the league average.
  9.  Calculates matchup advantages:
      - `off_advantage`: Team Avg OEFF - Opponent Avg DEFF
      - `def_advantage`: Opponent Avg OEFF - Team Avg DEFF
      - `pace_diff`: Team Avg PACE - Opponent Avg PACE
      - `orr_advantage`: Team Avg ORR - (1 - Opponent Avg DRR)
      - `drr_advantage`: Team Avg DRR - (1 - Opponent Avg ORR)
  10. Prints a season summary ranking teams by their average predicted adjusted net rating.
- **Output:**
  - `../../data/team_level_stats/nba_YYYY_team_metrics.csv` (for each year from 2015 to 2024). Contains team-level stats enhanced with pre-game averages, opponent averages, and matchup metrics.

**4. `merge_player_season.py`**

- **Goal:** Merge the feature-engineered player data with the processed team-level data for each season.
- **Inputs:**
  - `../../data/feature_engineer/feature_engineered_YYYY.xlsx` (from step 2).
  - `../../data/team_level_stats/nba_YYYY_team_metrics.csv` (from step 3).
- **Actions:**
  1.  Iterates through the seasons 2015 to 2024.
  2.  Loads the corresponding player and team files for the season.
  3.  Renames team data columns (`GAME-ID`, `DATE`, `TEAM`) to match player data (`game_id`, `date`, `team_name`).
  4.  Drops redundant raw stats and intermediate calculation columns from the team data.
  5.  Performs validation checks before merging (key uniqueness, team name consistency).
  6.  Merges the player data with the team data using `game_id`, `date`, and `team_name` as keys. This adds the pre-game team/opponent averages and matchup context to each player row.
  7.  Performs validation checks after merging (e.g., checking for nulls in the newly merged columns).
- **Output:**
  - `../../data/feature_reduction/merged_YYYY.xlsx` (for each year from 2015 to 2024). This file combines player features and team/matchup context.

**5. `NBA_encoding.py`**

- **Goal:** Encode categorical features into numerical representations suitable for machine learning models. **\*Note:** As written, this script explicitly processes `merged_2024.xlsx` and outputs `final_final_2024.xlsx`. To process all seasons, this script would need to be adapted to loop through seasons 2015-2024, reading `merged_YYYY.xlsx` and writing `final_final_YYYY.xlsx`.\*
- **Inputs:**
  - `../../data/feature_reduction/merged_YYYY.xlsx` (e.g., `merged_2024.xlsx`).
  - `../../data/team_id_mapping.json`
  - `../../data/offensive_archetype_mapping.json`
  - `../../data/defensive_role_mapping.json`
  - `../../data/rotation_role_mapping.json`
- **Actions:**
  1.  Loads the `merged_YYYY.xlsx` dataset.
  2.  **Team Encoding:** Maps `team_name` and `opponent_team` to numerical IDs (`team_id`, `opponent_id`) using the `team_id_mapping.json`. Drops original columns.
  3.  **Binary Encoding:** Converts `venue(R/H)` and `starter(Y/N)` into binary 0/1 columns. Drops original columns.
  4.  **Position Encoding:** Encodes the `position` column (which can include multi-position designations like 'G-F') into three binary flags: `is_G`, `is_F`, `is_C`. Drops the original `position` column.
  5.  **Rest Days (Redundant):** Includes code to process `rest_days` again, similar to `feature_engineering.py`. This section might be unnecessary if the columns (`rest_days_numeric`, `extended_absence`) from step 2 are carried through.
  6.  **Role/Archetype Encoding:** Maps `Offensive Archetype`, `Defensive Role`, and `Rotation Role` to numerical IDs using their respective JSON mapping files. Updates mapping files if new categories are encountered. Drops original columns.
  7.  **Data Type Conversion:** Converts specific ID columns (`player_id`, `team_id`, `opponent_id`, archetype/role IDs) to the 'category' data type, which can be efficient for some modeling libraries.
  8.  Drops the `tm_id` column.
- **Output:**
  - `../../data/final_final_data/final_final_YYYY.xlsx` (e.g., `final_final_2024.xlsx`). Contains the fully encoded data for the season.

**6. `merged_total.py`**

- **Goal:** Combine the processed and encoded data from all individual seasons into a single large dataset.
- **Inputs:**
  - `../../data/final_final_data/final_final_YYYY.xlsx` (for each year from 2015 to 2024, generated by the previous step).
- **Actions:**
  1.  Loads all `final_final_YYYY.xlsx` dataframes.
  2.  Performs a check to ensure all dataframes have the same columns before concatenation.
  3.  Concatenates the individual seasonal dataframes into one `merged_df`.
  4.  Performs spot checks using known `date` and `player_id` pairs to verify data integrity after concatenation.
- **Output:**
  - `../../data/merged_data_for_model/merged_model_data.xlsx`. The final combined dataset across all processed seasons in Excel format.

**7. _Untitled Script_ (Let's call it `final_formatting_save.py`)**

- **Goal:** Final data type adjustments and saving the combined dataset in Parquet format.
- **Input:**
  - `../../data/merged_data_for_model/merged_model_data.xlsx` (from step 6).
- **Actions:**
  1.  Loads the combined Excel file.
  2.  Re-applies the 'category' data type to the specified ID columns (this might be redundant if saved correctly in step 6, but ensures correctness).
  3.  Checks if the `date` column is sorted chronologically (`is_monotonic_increasing`).
- **Output:**
  - `../../data/merged_data_for_model/merged_model_data_collab_2.parquet`. The final dataset saved in Parquet format, which is generally more efficient for storage and loading in data science workflows compared to Excel for large datasets.
